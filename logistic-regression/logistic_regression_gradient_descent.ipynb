{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`63070501061 S.RAKNA`\n",
    "\n",
    "# Logistic Regression Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2.5 hrs\n",
    "\n",
    "#### Modify `Logistic Regression Gradient Descent.ipynb` for Iris classification to use the cost function and Jacobian function and the `BFGS` optimization method in `SciPy` library to:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 20 points\n",
    "\n",
    "##### a. Report the total classification accuracy (score) for the Test data by finding the highest probability class as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, X_train, y_train, X_test, y_test):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "        self.theta = (\n",
    "            20 * np.random.random_sample((X_train.shape[1], 1)) - 10).flatten()\n",
    "        self.predict_y = None\n",
    "        self.predict_y_before = None\n",
    "        self.correct = None\n",
    "\n",
    "    def stable_sigmoid(self, x) -> np.ndarray:\n",
    "        return np.where(\n",
    "            x < 0,\n",
    "            np.exp(x)/(1 + np.exp(x)),\n",
    "            1/(1 + np.exp(-x))\n",
    "        )\n",
    "\n",
    "    def h(self, theta, X):  # theta is a n x 1 vector\n",
    "        # X is a mxn matrix\n",
    "        #  [x0 x1 ... x_n-1]_1 where n-1 is size of feature vector (no. features)\n",
    "        #  [x0 x1 ... x_n-1]_2 REMEMBER x0 is 1.\n",
    "        #  ...\n",
    "        #  [x0 x1 ... x_n-1]_m where m is number of data points\n",
    "        # for matrix X_mxn, and vector theta_nx1 this will return a vector mx1\n",
    "        # print(\"shape X, theta = \", X.shape, theta.shape)\n",
    "        z = X @ theta  # X is m x n, theta is n x 1. Result z is mx1.\n",
    "        return self.stable_sigmoid(z)  # returns vector mx1\n",
    "\n",
    "    def J(self, theta, X, y):\n",
    "        # X[i] is 1 data point\n",
    "        # y = X @ theta where X is m x n and theta is n x 1. y is m x 1.\n",
    "        # y[i] is 0 or 1 only for data belongs to this class or not this class\n",
    "        # m is number of data points.  n is number of features\n",
    "        # first column of X contain all 1's\n",
    "        # Assume we have J = (-1/m)SUM_{i=1..m}[y_i * log(h(theta, X[i])) +\n",
    "        #                                          (1-y_i)*log(1-h(theta, X[i])) ]\n",
    "        m = X.shape[0]  # m - number of data points\n",
    "        # n - no. features + 1, because first column X is all 1 for intercepts.\n",
    "        n = X.shape[1]\n",
    "        H = self.h(theta, X)\n",
    "        # print (\"In function E, m, n = \", m, n, \"y.shape = \", y.shape, \"h shape = \", T1.shape)\n",
    "        cost = -(y.T @ np.log(H) + (1-y).T @ np.log(1-H))/m\n",
    "        return cost  # returns a scalar\n",
    "\n",
    "    def gradient(self, theta, X, y):\n",
    "        m = X.shape[0]  # m - number of data points\n",
    "        # n - no. features + 1, because first column X is all 1 for intercepts.\n",
    "        n = X.shape[1]\n",
    "        # this is n x m * m x 1 to get n x 1\n",
    "        dJ_dt = X.T @ (self.h(theta, X) - y)/m\n",
    "        return dJ_dt  # an array n x 1\n",
    "\n",
    "    def minimize(self):\n",
    "        res = minimize(self.J, self.theta, args=(self.X_train, self.y_train),\n",
    "                       method='BFGS', jac=self.gradient, options={'disp': True})\n",
    "        self.theta = res.x\n",
    "\n",
    "    def predict(self):\n",
    "        self.predict_y_before = self.h(self.theta, self.X_test)\n",
    "        self.predict_y = np.where(self.predict_y_before >= 0.5, 1, 0)\n",
    "        # correct in percentage\n",
    "        self.correct = np.sum(self.predict_y == self.y_test) / \\\n",
    "            self.y_test.shape[0] * 100\n",
    "\n",
    "    def print(self):\n",
    "        print(\"theta = \", self.theta)\n",
    "        print(\"predict_y = \", self.predict_y)\n",
    "        print(\"correct = \", self.correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 samples Iris data X = \n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "All Iris data y = \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "First 5 samples of X_train: \n",
      " [[5.9 3.  4.2 1.5]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [6.9 3.1 5.1 2.3]]\n",
      "First 5 samples of y_train: \n",
      " [1 1 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "print(\"5 samples Iris data X = \\n\", X[:5][:])\n",
    "print(\"All Iris data y = \\n\", y)\n",
    "# Use sklearn's library to split the data into training and testing sets with ratio 75% to 25%.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0)\n",
    "print(\"First 5 samples of X_train: \\n\", X_train[0:5][:])\n",
    "print(\"First 5 samples of y_train: \\n\", y_train[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class\n",
    "y0_train = np.where(y_train == 0, 1, 0)\n",
    "y1_train = np.where(y_train == 1, 1, 0)\n",
    "y2_train = np.where(y_train == 2, 1, 0)\n",
    "y0_test = np.where(y_test == 0, 1, 0)\n",
    "y1_test = np.where(y_test == 1, 1, 0)\n",
    "y2_test = np.where(y_test == 2, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000002\n",
      "         Iterations: 22\n",
      "         Function evaluations: 29\n",
      "         Gradient evaluations: 29\n",
      "theta =  [  2.40932953   7.11747435 -13.04678157  -4.97442521]\n",
      "predict_y =  [0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1\n",
      " 0]\n",
      "correct =  100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dm/8jvmtg054t9fvkdq5lplxb200000gn/T/ipykernel_13306/2387489400.py:45: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -(y.T @ np.log(H) + (1-y).T @ np.log(1-H))/m\n",
      "/var/folders/dm/8jvmtg054t9fvkdq5lplxb200000gn/T/ipykernel_13306/2387489400.py:45: RuntimeWarning: invalid value encountered in matmul\n",
      "  cost = -(y.T @ np.log(H) + (1-y).T @ np.log(1-H))/m\n"
     ]
    }
   ],
   "source": [
    "lr0 = LogisticRegression(X_train, y0_train, X_test, y0_test)\n",
    "lr0.minimize()\n",
    "lr0.predict()\n",
    "lr0.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.510000\n",
      "         Iterations: 29\n",
      "         Function evaluations: 37\n",
      "         Gradient evaluations: 37\n",
      "theta =  [ 0.6721146  -1.79056047  1.10285494 -3.07298673]\n",
      "predict_y =  [0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n",
      "correct =  63.1578947368421\n"
     ]
    }
   ],
   "source": [
    "lr1 = LogisticRegression(X_train, y1_train, X_test, y1_test)\n",
    "lr1.minimize()\n",
    "lr1.predict()\n",
    "lr1.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: nan\n",
      "         Iterations: 21\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "theta =  [-14.64059462 -15.59542351  23.67060079  12.26399725]\n",
      "predict_y =  [1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0\n",
      " 1]\n",
      "correct =  97.36842105263158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dm/8jvmtg054t9fvkdq5lplxb200000gn/T/ipykernel_13306/2387489400.py:45: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -(y.T @ np.log(H) + (1-y).T @ np.log(1-H))/m\n",
      "/var/folders/dm/8jvmtg054t9fvkdq5lplxb200000gn/T/ipykernel_13306/2387489400.py:45: RuntimeWarning: invalid value encountered in matmul\n",
      "  cost = -(y.T @ np.log(H) + (1-y).T @ np.log(1-H))/m\n",
      "/var/folders/dm/8jvmtg054t9fvkdq5lplxb200000gn/T/ipykernel_13306/2387489400.py:45: RuntimeWarning: divide by zero encountered in log\n",
      "  cost = -(y.T @ np.log(H) + (1-y).T @ np.log(1-H))/m\n",
      "/var/folders/dm/8jvmtg054t9fvkdq5lplxb200000gn/T/ipykernel_13306/2387489400.py:45: RuntimeWarning: invalid value encountered in matmul\n",
      "  cost = -(y.T @ np.log(H) + (1-y).T @ np.log(1-H))/m\n"
     ]
    }
   ],
   "source": [
    "lr2 = LogisticRegression(X_train, y2_train, X_test, y2_test)\n",
    "lr2.minimize()\n",
    "lr2.predict()\n",
    "lr2.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class0:\n",
      "[4.38212945e-20 1.79300031e-12 1.00000000e+00 1.04034151e-23\n",
      " 9.99999845e-01 2.48220198e-23 9.99999991e-01 5.26646010e-14\n",
      " 3.53388749e-15 3.67074732e-11 4.62029378e-21 7.07861471e-13\n",
      " 6.52363061e-15 1.41754766e-14 4.91500836e-15 9.99999992e-01\n",
      " 3.19213254e-14 1.85482036e-14 9.99998073e-01 1.00000000e+00\n",
      " 2.69022569e-18 2.48110698e-14 9.99953705e-01 9.99993740e-01\n",
      " 1.13839096e-16 1.00000000e+00 9.99996474e-01 1.89956760e-12\n",
      " 3.03830589e-09 9.99998456e-01 1.68494380e-19 1.53240580e-14\n",
      " 9.99999953e-01 1.00752777e-16 7.38755586e-22 4.61850928e-12\n",
      " 9.99999963e-01 1.86269290e-18]\n",
      "\n",
      "class1:\n",
      "[0.05386458 0.80719525 0.05243486 0.75591662 0.15608546 0.06066161\n",
      " 0.08358745 0.38375706 0.63374856 0.37821459 0.78889008 0.25447369\n",
      " 0.64156515 0.45483053 0.44732381 0.12831299 0.30862175 0.55135593\n",
      " 0.17895692 0.04112083 0.12007588 0.22192688 0.20085992 0.21317011\n",
      " 0.25272501 0.05385845 0.07515207 0.4309144  0.45232489 0.10047486\n",
      " 0.32852188 0.19958415 0.15029609 0.19794796 0.21473457 0.20741724\n",
      " 0.11710666 0.47650624]\n",
      "\n",
      "class2:\n",
      "[1.00000000e+00 2.48679910e-07 1.09600953e-48 9.99999995e-01\n",
      " 4.62791405e-39 1.00000000e+00 2.91541927e-41 5.10746635e-08\n",
      " 3.97791292e-06 1.96725175e-10 9.99999998e-01 7.62783817e-09\n",
      " 9.05431957e-04 9.63314338e-06 2.20915082e-03 2.43218828e-41\n",
      " 2.86754745e-04 9.93980078e-02 1.41537405e-35 5.55629533e-46\n",
      " 9.99999648e-01 2.06352987e-02 1.11958146e-33 6.90057076e-33\n",
      " 7.78199810e-01 5.17738116e-43 3.14478949e-37 1.16072608e-08\n",
      " 7.59734256e-09 5.73619203e-37 9.99963490e-01 2.82563318e-01\n",
      " 5.20497630e-41 8.77314484e-01 1.00000000e+00 1.57835989e-04\n",
      " 1.24157515e-43 9.99969301e-01]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"class0:\\n{lr0.predict_y_before}\\n\")\n",
    "print(f\"class1:\\n{lr1.predict_y_before}\\n\")\n",
    "print(f\"class2:\\n{lr2.predict_y_before}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_class:\n",
      "[2, 1, 0, 2, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1, 0, 0, 2, 0, 0, 1, 1, 0, 2, 2, 0, 2, 2, 1, 0, 2]\n",
      "\n",
      "y_test:\n",
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 1]\n",
      "\n",
      "max_class == y_test:\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True False]\n",
      "\n",
      "correct:\n",
      "94.73684210526315\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_class = []\n",
    "for i in range(len(lr0.predict_y_before)):\n",
    "    max_class.append(np.argmax([\n",
    "        lr0.predict_y_before[i],\n",
    "        lr1.predict_y_before[i],\n",
    "        lr2.predict_y_before[i]\n",
    "    ]))\n",
    "\n",
    "print(f\"max_class:\\n{max_class}\\n\")\n",
    "print(f\"y_test:\\n{y_test}\\n\")\n",
    "print(f\"max_class == y_test:\\n{max_class == y_test}\\n\")\n",
    "print(f\"correct:\\n{np.sum(max_class == y_test) / len(y_test) * 100}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5 points\n",
    "\n",
    "##### b. Print the **confusion matrix** for the test data of the 3 classes found by your own implementation of the logistic regressor. You can use `Sklearn`’s library; here’s an example of how:\n",
    "<image style=\"width:50%\" src=\"./1b-ex.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix:\n",
      "[[13  0  0]\n",
      " [ 0 14  2]\n",
      " [ 0  0  9]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(f\"confusion_matrix:\\n{confusion_matrix(y_test, max_class)}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 5 points\n",
    "\n",
    "##### c. Print the confusion matrix for the test data found by `sklearn`’s logistic regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regressor = LogisticRegression(max_iter=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred:\n",
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2]\n",
      "\n",
      "y_test:\n",
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 1]\n",
      "\n",
      "y_pred == y_test:\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False]\n",
      "\n",
      "correct:\n",
      "97.36842105263158\n",
      "\n",
      "confusion_matrix:\n",
      "[[13  0  0]\n",
      " [ 0 15  1]\n",
      " [ 0  0  9]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fit_class = logistic_regressor.fit(X_train, y_train)\n",
    "y_pred = fit_class.predict(X_test)\n",
    "print(f\"y_pred:\\n{y_pred}\\n\")\n",
    "print(f\"y_test:\\n{y_test}\\n\")\n",
    "print(f\"y_pred == y_test:\\n{y_pred == y_test}\\n\")\n",
    "print(f\"correct:\\n{np.sum(y_pred == y_test) / len(y_test) * 100}\\n\")\n",
    "print(f\"confusion_matrix:\\n{confusion_matrix(y_test, y_pred)}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 10 points\n",
    "\n",
    "##### d. For each **mis-classified** data sample, show the classification probability for class 0, 1, and 2 \n",
    "*we want to see that the probabilities are hovering around the 0.2 to 0.9 range for these data points*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction probability for test data P[0], P[1], P[2]: \n",
      " [[1.16879713e-04 5.59173946e-02 9.43965726e-01]\n",
      " [1.26309415e-02 9.60290623e-01 2.70784360e-02]\n",
      " [9.84388468e-01 1.56114930e-02 3.88752756e-08]\n",
      " [1.25811004e-06 2.34527966e-02 9.76545945e-01]\n",
      " [9.70274628e-01 2.97252070e-02 1.64616041e-07]\n",
      " [2.00954229e-06 5.97993123e-03 9.94018059e-01]\n",
      " [9.81934830e-01 1.80650983e-02 7.13675998e-08]\n",
      " [2.83358621e-03 7.47763906e-01 2.49402507e-01]\n",
      " [1.50589964e-03 7.39154135e-01 2.59339965e-01]\n",
      " [2.04941217e-02 9.35763785e-01 4.37420934e-02]\n",
      " [9.19538183e-05 1.60144101e-01 8.39763946e-01]\n",
      " [6.95834502e-03 8.10288425e-01 1.82753230e-01]\n",
      " [4.06402244e-03 7.93855231e-01 2.02080746e-01]\n",
      " [3.04877992e-03 7.60967799e-01 2.35983421e-01]\n",
      " [3.85935596e-03 7.10467056e-01 2.85673588e-01]\n",
      " [9.82822812e-01 1.71771312e-02 5.72475965e-08]\n",
      " [6.69996306e-03 7.56228367e-01 2.37071670e-01]\n",
      " [1.13766781e-02 8.44682142e-01 1.43941180e-01]\n",
      " [9.67687107e-01 3.23126753e-02 2.17465690e-07]\n",
      " [9.82895242e-01 1.71046979e-02 6.03076896e-08]\n",
      " [8.27531372e-04 1.92657570e-01 8.06514899e-01]\n",
      " [1.02634474e-02 7.10762083e-01 2.78974470e-01]\n",
      " [9.44083677e-01 5.59152612e-02 1.06151573e-06]\n",
      " [9.75560224e-01 2.44396061e-02 1.70402003e-07]\n",
      " [1.36301822e-03 4.26065113e-01 5.72571869e-01]\n",
      " [9.94212558e-01 5.78743237e-03 9.80367798e-09]\n",
      " [9.50168673e-01 4.98301825e-02 1.14464264e-06]\n",
      " [1.06791342e-02 9.00956616e-01 8.83642501e-02]\n",
      " [1.40914255e-01 8.52760070e-01 6.32567558e-03]\n",
      " [9.61542435e-01 3.84571100e-02 4.55360666e-07]\n",
      " [9.88493616e-05 1.16264163e-01 8.83636987e-01]\n",
      " [1.19058905e-02 6.83821341e-01 3.04272768e-01]\n",
      " [9.68102636e-01 3.18972124e-02 1.51800370e-07]\n",
      " [1.27874111e-03 3.57864466e-01 6.40856793e-01]\n",
      " [1.48297671e-05 3.39019475e-02 9.66083223e-01]\n",
      " [4.79743554e-02 8.80299983e-01 7.17256618e-02]\n",
      " [9.44664156e-01 5.53354496e-02 3.94013572e-07]\n",
      " [5.99618425e-04 3.11178852e-01 6.88221529e-01]]\n"
     ]
    }
   ],
   "source": [
    "y_test_prob = fit_class.predict_proba(X_test)\n",
    "print(\"Prediction probability for test data P[0], P[1], P[2]: \\n\", y_test_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction probability for 5 rows of data P[0], P[1], P[2]: \n",
      " [[0.   0.06 0.94]\n",
      " [0.01 0.96 0.03]\n",
      " [0.98 0.02 0.  ]\n",
      " [0.   0.02 0.98]\n",
      " [0.97 0.03 0.  ]\n",
      " [0.   0.01 0.99]\n",
      " [0.98 0.02 0.  ]\n",
      " [0.   0.75 0.25]\n",
      " [0.   0.74 0.26]\n",
      " [0.02 0.94 0.04]\n",
      " [0.   0.16 0.84]\n",
      " [0.01 0.81 0.18]\n",
      " [0.   0.79 0.2 ]\n",
      " [0.   0.76 0.24]\n",
      " [0.   0.71 0.29]\n",
      " [0.98 0.02 0.  ]\n",
      " [0.01 0.76 0.24]\n",
      " [0.01 0.84 0.14]\n",
      " [0.97 0.03 0.  ]\n",
      " [0.98 0.02 0.  ]\n",
      " [0.   0.19 0.81]\n",
      " [0.01 0.71 0.28]\n",
      " [0.94 0.06 0.  ]\n",
      " [0.98 0.02 0.  ]\n",
      " [0.   0.43 0.57]\n",
      " [0.99 0.01 0.  ]\n",
      " [0.95 0.05 0.  ]\n",
      " [0.01 0.9  0.09]\n",
      " [0.14 0.85 0.01]\n",
      " [0.96 0.04 0.  ]\n",
      " [0.   0.12 0.88]\n",
      " [0.01 0.68 0.3 ]\n",
      " [0.97 0.03 0.  ]\n",
      " [0.   0.36 0.64]\n",
      " [0.   0.03 0.97]\n",
      " [0.05 0.88 0.07]\n",
      " [0.94 0.06 0.  ]\n",
      " [0.   0.31 0.69]]\n"
     ]
    }
   ],
   "source": [
    "d = 2\n",
    "# print with d decimal places and suppress scientific notation\n",
    "np.set_printoptions(precision=d, suppress=True)\n",
    "print(\n",
    "    \"Prediction probability for 5 rows of data P[0], P[1], P[2]: \\n\", y_test_prob)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
